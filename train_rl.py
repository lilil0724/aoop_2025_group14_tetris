import torch
import torch.nn as nn
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.callbacks import CheckpointCallback
import math
import os

import config
import tetris_env
import copy

# -------------------------------------------------
# 0. Transformer æ¨¡å‹å®šç¾© (å¿…é ˆèˆ‡è¨“ç·´æ™‚å®Œå…¨ä¸€è‡´)
# -------------------------------------------------

class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 512):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(1)
        self.register_buffer("pe", pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        seq_len = x.size(0)
        return x + self.pe[:seq_len]

class TetrisTransformer(nn.Module):
    def __init__(self, board_dim: int = 200, n_pieces: int = 7, d_model: int = 128, nhead: int = 4, num_layers: int = 3, action_dim: int = 64):
        super().__init__()
        self.board_proj = nn.Linear(board_dim, d_model)
        self.piece_emb = nn.Embedding(n_pieces, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len=2)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=4 * d_model, dropout=0.1, batch_first=False)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.action_head = nn.Linear(d_model, action_dim)

    def forward(self, board_flat: torch.Tensor, piece_id: torch.Tensor) -> torch.Tensor:
        board_token = self.board_proj(board_flat)
        piece_token = self.piece_emb(piece_id)
        tokens = torch.stack([piece_token, board_token], dim=0)
        tokens = self.pos_encoder(tokens)
        output = self.transformer(tokens)
        cls_token = output[0]
        logits = self.action_head(cls_token)
        return logits

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -------------------------------------------------
# 1. å®šç¾© Gym ç’°å¢ƒ
# -------------------------------------------------
class TetrisGymEnv(gym.Env):
    def __init__(self):
        super().__init__()
        self.env = tetris_env.TetrisEnv()
        
        # è§€å¯Ÿç©ºé–“: [piece_id (1) + board (200)] = 201
        self.observation_space = spaces.Box(low=0, high=7, shape=(201,), dtype=np.float32)
        
        # å‹•ä½œç©ºé–“: 64 å€‹é›¢æ•£å‹•ä½œ
        self.action_space = spaces.Discrete(64) 

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.env.reset()
        return self._get_obs(), {}

    def step(self, action_id):
        # è§£ç¢¼å‹•ä½œ (Action ID -> x, rot)
        max_rot = 4
        min_x = -2
        max_x = config.columns + 3
        num_x = max_x - min_x + 1
        
        rot = action_id // num_x
        x_idx = action_id % num_x
        x = x_idx + min_x
        
        # åŸ·è¡Œå‹•ä½œ
        # æ³¨æ„ï¼šé€™è£¡æˆ‘å€‘ä¾è³´ tetris_env å…§éƒ¨çš„ step
        # å¦‚æœ env.step å›å‚³çš„ reward å·²ç¶“åŒ…å«æ¶ˆè¡Œçå‹µï¼Œé‚£å¾ˆå¥½
        original_reward, game_over = self.env.step((x, rot))
        
        rl_reward = original_reward
        
        # [å¼·åŒ–çå‹µæ©Ÿåˆ¶]
        if game_over:
            rl_reward = -100.0  # æ­»äº¡é‡ç½°
        else:
            # ç”Ÿå­˜çå‹µ (é¼“å‹µæ´»ä¸‹å»)
            rl_reward += 0.5
            
            # æˆ‘å€‘å¯ä»¥é¡å¤–çå‹µæ¶ˆè¡Œ (å¦‚æœ original_reward å·²ç¶“æœ‰ï¼Œé€™è¡Œå¯ä»¥çœç•¥)
            # å‡è¨­ env.line_count æœƒåœ¨ step å¾Œæ›´æ–°
            # rl_reward += self.env.last_cleared_lines * 10.0 
            
        # æˆªæ–· (Truncated): é€™è£¡æš«æ™‚ä¸ä½¿ç”¨æ­¥æ•¸æˆªæ–·ï¼Œè®“å®ƒè‡ªç„¶æ­»äº¡
        truncated = False
        
        return self._get_obs(), rl_reward, game_over, truncated, {}

    def _get_obs(self):
        # å–å¾—ç›¤é¢ (200ç¶­)
        board_np = (self.env.board == 2).astype(np.float32).flatten()
        
        # å–å¾—æ–¹å¡Š ID
        shape_list = list(config.shapes.keys())
        piece_id = shape_list.index(self.env.current_piece.shape)
        
        # æ‹¼æ¥
        obs = np.concatenate(([piece_id], board_np))
        return obs

# -------------------------------------------------
# 2. ç‰¹å¾µæå–å™¨ (è¼‰å…¥é è¨“ç·´æ¬Šé‡)
# -------------------------------------------------
class TransformerExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space: spaces.Box, features_dim: int = 128):
        super().__init__(observation_space, features_dim)
        
        # å»ºç«‹ Transformer
        self.transformer = TetrisTransformer(
            board_dim=200, n_pieces=7, d_model=128, 
            nhead=4, num_layers=3, action_dim=64
        )
        
        # è¼‰å…¥é è¨“ç·´æ¬Šé‡
        pretrained_path = "transformer_tetris.pth"
        if os.path.exists(pretrained_path):
            try:
                print(f"ğŸ”„ æ­£åœ¨è¼‰å…¥é è¨“ç·´æ¬Šé‡: {pretrained_path} ...")
                pretrained_dict = torch.load(pretrained_path, map_location=DEVICE)
                
                # éæ¿¾æ‰ action_head (å› ç‚º PPO æœƒè‡ªå·±å»ºç«‹æ–°çš„ Policy Head)
                model_dict = self.transformer.state_dict()
                pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and 'action_head' not in k}
                
                model_dict.update(pretrained_dict)
                self.transformer.load_state_dict(model_dict)
                print("âœ… æˆåŠŸè¼‰å…¥ Transformer ç‰¹å¾µæå–å±¤ï¼(Transfer Learning)")
                
                # å¯é¸ï¼šå‡çµ Transformer æ¬Šé‡ï¼Œåªè¨“ç·´ Policy Head (å…ˆç·´æ‰‹è…³)
                # for param in self.transformer.parameters():
                #     param.requires_grad = False
                # print("â„ï¸ Transformer æ¬Šé‡å·²å‡çµ")
                
            except Exception as e:
                print(f"âš ï¸ æ¬Šé‡è¼‰å…¥å¤±æ•—: {e}ï¼Œå°‡å¾é ­è¨“ç·´ã€‚")
        else:
            print("âš ï¸ æ‰¾ä¸åˆ°é è¨“ç·´æ¬Šé‡ï¼Œå°‡å¾é ­è¨“ç·´ã€‚")

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        # observations: [Batch, 201]
        piece_id = observations[:, 0].long()
        board_flat = observations[:, 1:]
        
        # æ‰‹å‹•åŸ·è¡Œ Transformer å‰åŠæ®µ
        board_token = self.transformer.board_proj(board_flat)
        piece_token = self.transformer.piece_emb(piece_id)
        
        tokens = torch.stack([piece_token, board_token], dim=0)
        tokens = self.transformer.pos_encoder(tokens)
        
        output = self.transformer.transformer(tokens)
        cls_token = output[0] # [Batch, 128]
        
        return cls_token

# -------------------------------------------------
# 3. ä¸»è¨“ç·´æµç¨‹
# -------------------------------------------------
def train_rl():
    print(f"ğŸ”¥ å•Ÿå‹• RL å¼·åŒ–å­¸ç¿’è¨“ç·´ | Device: {DEVICE}")
    
    # å»ºç«‹ç’°å¢ƒ
    env = TetrisGymEnv()
    
    # å®šç¾© Checkpoint (æ¯ 50000 æ­¥å­˜ä¸€æ¬¡)
    checkpoint_callback = CheckpointCallback(
        save_freq=50000,
        save_path="./rl_checkpoints/",
        name_prefix="ppo_tetris"
    )
    
    # PPO è¨­å®š
    model = PPO(
        "MlpPolicy", 
        env, 
        verbose=1,
        device=DEVICE,
        policy_kwargs={
            "features_extractor_class": TransformerExtractor,
            "features_extractor_kwargs": {"features_dim": 128}, 
            "net_arch": dict(pi=[64, 64], vf=[64, 64]) # Policy Head & Value Head
        },
        learning_rate=1e-5, # é™ä½å­¸ç¿’ç‡ï¼Œä¿è­·é è¨“ç·´æ¬Šé‡
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        ent_coef=0.01,      # å¢åŠ ç†µï¼Œé¼“å‹µæ¢ç´¢
    )
    
    print("ğŸš€ é–‹å§‹è¨“ç·´ (Target: 1M steps)...")
    try:
        model.learn(total_timesteps=1000000, callback=checkpoint_callback)
    except KeyboardInterrupt:
        print("ğŸ›‘ è¨“ç·´è¢«æ‰‹å‹•ä¸­æ–·")
    
    # æœ€çµ‚å­˜æª”
    model.save("ppo_transformer_tetris_final")
    print("ğŸ’¾ æœ€çµ‚ RL æ¨¡å‹å·²å„²å­˜ç‚º ppo_transformer_tetris_final.zip")

if __name__ == "__main__":
    # å¦‚æœä½ æƒ³å¾é ­ç·´ï¼Œå°±å‘¼å« train_rl()
    # train_rl()
    
    # å¦‚æœä½ æƒ³æ¥çºŒç·´ï¼Œå°±ç”¨é€™æ®µï¼š
    model_path = "ppo_transformer_tetris_final.zip" # ä¸Šæ¬¡å­˜çš„æª”
    if os.path.exists(model_path):
        print(f"ğŸ”„ è¼‰å…¥ {model_path} ç¹¼çºŒè¨“ç·´...")
        env = TetrisGymEnv()
        model = PPO.load(model_path, env=env, device=DEVICE)
        model.learn(total_timesteps=100000, reset_num_timesteps=False)
        model.save("ppo_transformer_tetris_continued")
        print("ğŸ’¾ çºŒç·´å®Œæˆä¸¦å­˜æª”")
    else:
        print("âŒ æ‰¾ä¸åˆ°èˆŠæª”ï¼Œé–‹å§‹æ–°è¨“ç·´")
        train_rl()