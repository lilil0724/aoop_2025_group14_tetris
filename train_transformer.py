import os
import math
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np

from dataset import TetrisDataset 

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -----------------------------
# 1. æ¨¡å‹å®šç¾©
# -----------------------------
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 512):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(1)
        self.register_buffer("pe", pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        seq_len = x.size(0)
        return x + self.pe[:seq_len]

class TetrisTransformer(nn.Module):
    def __init__(self, board_dim: int = 200, n_pieces: int = 7, d_model: int = 128, nhead: int = 4, num_layers: int = 3, action_dim: int = 64):
        super().__init__()
        self.board_proj = nn.Linear(board_dim, d_model)
        self.piece_emb = nn.Embedding(n_pieces, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len=2)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=4 * d_model, dropout=0.1, batch_first=False)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.action_head = nn.Linear(d_model, action_dim)

    def forward(self, board_flat: torch.Tensor, piece_id: torch.Tensor) -> torch.Tensor:
        board_token = self.board_proj(board_flat)
        piece_token = self.piece_emb(piece_id)
        tokens = torch.stack([piece_token, board_token], dim=0)
        tokens = self.pos_encoder(tokens)
        output = self.transformer(tokens)
        cls_token = output[0]
        logits = self.action_head(cls_token)
        return logits

# -----------------------------
# 2. è¨“ç·´è¼”åŠ©å‡½å¼
# -----------------------------
def collate_fn(batch):
    boards = []
    piece_ids = []
    action_ids = []
    for sample in batch:
        board = sample["board"]
        piece = sample["piece_id"]
        action = sample["action_id"]
        boards.append(board.reshape(-1))
        piece_ids.append(piece)
        action_ids.append(action)
    boards_t = torch.tensor(np.stack(boards), dtype=torch.float32)
    piece_ids_t = torch.tensor(piece_ids, dtype=torch.long)
    action_ids_t = torch.tensor(action_ids, dtype=torch.long)
    return boards_t, piece_ids_t, action_ids_t

# -----------------------------
# 3. ä¸»è¨“ç·´è¿´åœˆ (åŠ å…¥çºŒç·´åŠŸèƒ½)
# -----------------------------
def train(
    dataset_path: str = "tetris_demo_data.npz",
    save_path: str = "transformer_tetris.pth",
    epochs: int = 50,
    batch_size: int = 128,
    lr: float = 1e-4,
    resume: bool = True # æ–°å¢é–‹é—œï¼šæ˜¯å¦è¦è¼‰å…¥èˆŠæ¨¡å‹
):
    print(f"ğŸ”¥ é–‹å§‹è¨“ç·´ Transformer | Device: {DEVICE}")
    
    if not os.path.exists(dataset_path):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è³‡æ–™é›† {dataset_path}")
        return

    dataset = TetrisDataset(dataset_path)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)
    print(f"ğŸ“Š è³‡æ–™ç­†æ•¸: {len(dataset)}")

    # å»ºç«‹æ¨¡å‹
    model = TetrisTransformer().to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # --- çºŒç·´é‚è¼¯ ---
    start_epoch = 1
    if resume and os.path.exists(save_path):
        print(f"ğŸ”„ ç™¼ç¾æ—¢æœ‰æ¨¡å‹ {save_path}ï¼Œæ­£åœ¨è¼‰å…¥ä»¥ç¹¼çºŒè¨“ç·´...")
        try:
            # å¦‚æœä½ æœ‰å­˜ optimizer state æ›´å¥½ï¼Œé€™è£¡ç°¡åŒ–åªè¼‰å…¥æ¬Šé‡
            # é€™æ¨£ optimizer çš„ momentum æœƒé‡ç½®ï¼Œä½†å°å¾®èª¿å½±éŸ¿ä¸å¤§
            model.load_state_dict(torch.load(save_path, map_location=DEVICE))
            print("âœ… æˆåŠŸè¼‰å…¥èˆŠæ¬Šé‡ï¼")
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥å¤±æ•— ({e})ï¼Œå°‡é‡æ–°é–‹å§‹è¨“ç·´ã€‚")
    else:
        print("ğŸ†• æ‰¾ä¸åˆ°èˆŠæ¨¡å‹æˆ– resume=Falseï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´ã€‚")

    # è¨“ç·´è¿´åœˆ
    for epoch in range(start_epoch, epochs + 1):
        model.train()
        total_loss = 0
        total_correct = 0
        total_samples = 0

        for boards, piece_ids, action_ids in dataloader:
            boards = boards.to(DEVICE)
            piece_ids = piece_ids.to(DEVICE)
            action_ids = action_ids.to(DEVICE)

            optimizer.zero_grad()
            logits = model(boards, piece_ids)
            loss = criterion(logits, action_ids)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item() * boards.size(0)
            preds = logits.argmax(dim=1)
            total_correct += (preds == action_ids).sum().item()
            total_samples += boards.size(0)

        avg_loss = total_loss / total_samples
        acc = total_correct / total_samples

        print(f"Epoch {epoch:03d}/{epochs} | Loss: {avg_loss:.4f} | Acc: {acc*100:.2f}%")

        if epoch % 10 == 0:
            torch.save(model.state_dict(), save_path)
            print(f"ğŸ’¾ æ¨¡å‹å·²å‚™ä»½è‡³ {save_path}")

    torch.save(model.state_dict(), save_path)
    print(f"ğŸ‰ è¨“ç·´å®Œæˆï¼æœ€çµ‚æ¨¡å‹: {save_path}")

if __name__ == "__main__":
    train(
        dataset_path="tetris_demo_data.npz", 
        epochs=500,        
        batch_size=256,
        lr=1e-4,
        resume=True # è¨­å®šç‚º True å³å¯çºŒç·´
    )
