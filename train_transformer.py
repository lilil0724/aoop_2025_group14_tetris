import os
import math
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np

# ç¢ºä¿é€™è£¡ import æ­£ç¢ºï¼Œå°æ‡‰æˆ‘å€‘å‰›å¯«å¥½çš„ dataset.py
from dataset import TetrisDataset 

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -----------------------------
# 1. æ¨¡å‹å®šç¾© (å…§å»ºåœ¨æ­¤æª”æ¡ˆä¸­ï¼Œæ–¹ä¾¿ç®¡ç†)
# -----------------------------

class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 512):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(1)
        self.register_buffer("pe", pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        seq_len = x.size(0)
        return x + self.pe[:seq_len]

class TetrisTransformer(nn.Module):
    """
    Transformer æ¨¡å‹ï¼š
    è¼¸å…¥: Flattenå¾Œçš„ç›¤é¢ (200ç¶­) + æ–¹å¡ŠID
    è¼¸å‡º: Action ID (0~63)
    """
    def __init__(
        self,
        board_dim: int = 200,      # 20x10 flatten
        n_pieces: int = 7,         # 7 ç¨® Tetromino
        d_model: int = 128,
        nhead: int = 4,
        num_layers: int = 3,
        action_dim: int = 64       # å‹•ä½œç©ºé–“å¤§å°
    ):
        super().__init__()
        self.board_proj = nn.Linear(board_dim, d_model)
        self.piece_emb = nn.Embedding(n_pieces, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len=2)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=4 * d_model,
            dropout=0.1,
            batch_first=False # (Seq, Batch, Dim)
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.action_head = nn.Linear(d_model, action_dim)

    def forward(self, board_flat: torch.Tensor, piece_id: torch.Tensor) -> torch.Tensor:
        # board_flat: (batch, 200)
        # piece_id: (batch,)
        
        board_token = self.board_proj(board_flat)       # (batch, d_model)
        piece_token = self.piece_emb(piece_id)          # (batch, d_model)

        # æ§‹å»ºåºåˆ—: [Piece, Board] -> (seq=2, batch, d_model)
        tokens = torch.stack([piece_token, board_token], dim=0)
        tokens = self.pos_encoder(tokens)

        output = self.transformer(tokens) # (seq=2, batch, d_model)
        
        # å–å‡ºç¬¬ä¸€å€‹ token (Piece token) ä½œç‚ºæ±ºç­–ç‰¹å¾µ
        cls_token = output[0] # (batch, d_model)
        
        logits = self.action_head(cls_token) # (batch, action_dim)
        return logits

# -----------------------------
# 2. è¨“ç·´è¼”åŠ©å‡½å¼
# -----------------------------

def collate_fn(batch):
    """
    æ•´ç† DataLoader çš„ batch
    batch æ˜¯ list of dict: [{'board':..., 'piece_id':..., 'action_id':...}, ...]
    """
    boards = []
    piece_ids = []
    action_ids = []

    for sample in batch:
        board = sample["board"]  # (20, 10)
        piece = sample["piece_id"]
        action = sample["action_id"]

        boards.append(board.reshape(-1)) # Flatten -> (200,)
        piece_ids.append(piece)
        action_ids.append(action)

    # è½‰æˆ Tensor
    boards_t = torch.tensor(np.stack(boards), dtype=torch.float32)
    piece_ids_t = torch.tensor(piece_ids, dtype=torch.long)
    action_ids_t = torch.tensor(action_ids, dtype=torch.long)
    
    return boards_t, piece_ids_t, action_ids_t

# -----------------------------
# 3. ä¸»è¨“ç·´è¿´åœˆ
# -----------------------------

def train(
    dataset_path: str = "tetris_demo_data.npz",
    save_path: str = "transformer_tetris.pth",
    epochs: int = 50,
    batch_size: int = 128,
    lr: float = 1e-4
):
    print(f"ğŸ”¥ é–‹å§‹è¨“ç·´ Transformer | Device: {DEVICE}")
    
    # 1. è®€å–è³‡æ–™
    if not os.path.exists(dataset_path):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è³‡æ–™é›† {dataset_path}")
        print("è«‹å…ˆåŸ·è¡Œ 'python dataset.py' ä¾†æ”¶é›†è³‡æ–™ï¼")
        return

    dataset = TetrisDataset(dataset_path)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn, # ä½¿ç”¨æˆ‘å€‘å®šç¾©çš„æ•´ç†å‡½å¼
        num_workers=0
    )
    print(f"ğŸ“Š è³‡æ–™ç­†æ•¸: {len(dataset)}")

    # 2. å»ºç«‹æ¨¡å‹
    model = TetrisTransformer().to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # 3. è¨“ç·´
    for epoch in range(1, epochs + 1):
        model.train()
        total_loss = 0
        total_correct = 0
        total_samples = 0

        for boards, piece_ids, action_ids in dataloader:
            boards = boards.to(DEVICE)
            piece_ids = piece_ids.to(DEVICE)
            action_ids = action_ids.to(DEVICE)

            optimizer.zero_grad()
            
            # å‰å‘å‚³æ’­
            logits = model(boards, piece_ids)
            
            # è¨ˆç®— Loss
            loss = criterion(logits, action_ids)
            
            # åå‘å‚³æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # çµ±è¨ˆ
            total_loss += loss.item() * boards.size(0)
            preds = logits.argmax(dim=1)
            total_correct += (preds == action_ids).sum().item()
            total_samples += boards.size(0)

        avg_loss = total_loss / total_samples
        acc = total_correct / total_samples

        print(f"Epoch {epoch:03d}/{epochs} | Loss: {avg_loss:.4f} | Acc: {acc*100:.2f}%")

        # å®šæœŸå­˜æª”
        if epoch % 10 == 0:
            torch.save(model.state_dict(), save_path)
            print(f"ğŸ’¾ æ¨¡å‹å·²å‚™ä»½è‡³ {save_path}")

    # æœ€çµ‚å­˜æª”
    torch.save(model.state_dict(), save_path)
    print(f"ğŸ‰ è¨“ç·´å®Œæˆï¼æœ€çµ‚æ¨¡å‹: {save_path}")

if __name__ == "__main__":
    # é€™è£¡è¨­å®šä½ çš„åƒæ•¸
    train(
        dataset_path="tetris_demo_data.npz", 
        epochs=100,        # æƒ³è¦ç·´ä¹…ä¸€é»å¯ä»¥æ”¹é€™è£¡
        batch_size=256,
        lr=1e-4
    )
